{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwA15NT8lF7w"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Information Bottleneck (IB) Trajectory Verification via N-Sweep (Multi-Run Version)\n",
        "\n",
        "This script implements a novel and theoretically sound experiment to demonstrate\n",
        "the Information Bottleneck phenomenon. This version is enhanced to run the\n",
        "entire experiment multiple times with different seeds to ensure the statistical\n",
        "robustness of the findings.\n",
        "\n",
        "Hypothesis (v3 - The Correct Interpretation):\n",
        "The IB dynamic does not correspond to a monotonic increase in data (D), but\n",
        "rather to a change in the internal resources (N) allocated to a concept.\n",
        "- The Fitting Phase is analogous to INCREASING the model capacity (N).\n",
        "- The Compression Phase is analogous to subsequently DECREASING the model capacity (N).\n",
        "\n",
        "This script simulates this by training a series of models with\n",
        "capacities (N) that first increase and then decrease, repeating this entire\n",
        "process for multiple runs to ensure the resulting trajectory is a robust phenomenon.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "import copy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "CONFIG = {\n",
        "    \"base_seed\": 42,\n",
        "    \"num_runs\": 5, # Number of independent runs to perform\n",
        "    \"fixed_epochs\": 40,\n",
        "    \"dataset_size\": 10000,\n",
        "    \"hidden_size_sweep\": [4, 6, 8, 12, 16, 24, 32, 48, 64, 96, 128, 96, 64, 48, 32, 24, 16, 12, 8, 6, 4],\n",
        "    \"batch_size\": 256,\n",
        "    \"learning_rate\": 0.002,\n",
        "    \"analysis_sample_size\": 50,\n",
        "    \"results_filename\": \"n_sweep_ib_results_multirun.json\",\n",
        "}\n",
        "\n",
        "# --- 2. Model and Theory Analyzer Definition (remains the same) ---\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_size=10, num_classes=10):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x.view(-1, 784))\n",
        "\n",
        "class TheoryAnalyzer:\n",
        "    def __init__(self, model):\n",
        "        model_copy = copy.deepcopy(model); model_copy.eval()\n",
        "        self.model = model_copy.to('cpu')\n",
        "        self.linear_layers = [m for m in self.model.modules() if isinstance(m, nn.Linear)]\n",
        "        self.graph = self._build_graph()\n",
        "        self.grounding_nodes = self._get_grounding_nodes()\n",
        "        self.hidden_nodes = self._get_hidden_nodes()\n",
        "        self.memoized_paths = {}\n",
        "    def _build_graph(self):\n",
        "        G = nx.DiGraph(); layers = self.linear_layers;\n",
        "        if not layers: return G\n",
        "        for i in range(layers[0].in_features): G.add_node(f\"0-{i}\", layer=0)\n",
        "        for i, l in enumerate(layers):\n",
        "            for j in range(l.out_features): G.add_node(f\"{i+1}-{j}\", layer=i+1)\n",
        "            weights = torch.abs(l.weight.data.t()); probs = torch.softmax(weights, dim=1)\n",
        "            for u in range(l.in_features):\n",
        "                for v in range(l.out_features):\n",
        "                    p = probs[u, v].item()\n",
        "                    if p > 1e-9: G.add_edge(f\"{i}-{u}\", f\"{i+1}-{v}\", cost=1.0 - np.log(p))\n",
        "        return G\n",
        "    def _get_grounding_nodes(self):\n",
        "        num_linear_layers = len(self.linear_layers)\n",
        "        return {node for node, data in self.graph.nodes(data=True) if data['layer'] == num_linear_layers}\n",
        "    def _get_hidden_nodes(self):\n",
        "        num_linear_layers = len(self.linear_layers)\n",
        "        return [node for node, data in self.graph.nodes(data=True) if data['layer'] in range(1, num_linear_layers)]\n",
        "    def find_all_paths_dfs(self, start, targets):\n",
        "        memo_key = (start, tuple(sorted(list(targets))))\n",
        "        if memo_key in self.memoized_paths: return self.memoized_paths[memo_key]\n",
        "        paths, stack = [], [(start, [start], 0)]\n",
        "        while stack:\n",
        "            curr, path, cost = stack.pop()\n",
        "            if curr in targets: paths.append({'path': path, 'cost': cost}); continue\n",
        "            if len(path) > (len(self.linear_layers) + 2): continue\n",
        "            for neighbor in self.graph.neighbors(curr):\n",
        "                if neighbor not in path:\n",
        "                    new_cost = cost + self.graph[curr][neighbor]['cost']\n",
        "                    stack.append((neighbor, path + [neighbor], new_cost))\n",
        "        self.memoized_paths[memo_key] = paths; return paths\n",
        "    def calculate_metrics_for_node(self, node):\n",
        "        paths = self.find_all_paths_dfs(node, self.grounding_nodes)\n",
        "        if not paths: return float('inf'), float('inf')\n",
        "        costs = np.array([p['cost'] for p in paths])\n",
        "        importances = np.exp(-1.0 * costs); conductances = 1.0 / costs\n",
        "        h_tse = 1.0 / np.sum(conductances) if np.sum(conductances) > 0 else float('inf')\n",
        "        total_importance = np.sum(importances)\n",
        "        probabilities = importances / total_importance if total_importance > 0 else importances\n",
        "        h_sie = -np.sum(probabilities * np.log2(probabilities + 1e-9))\n",
        "        return h_tse, h_sie\n",
        "    def analyze_model_state(self, sample_size):\n",
        "        htse_vals, hsie_vals = [], []\n",
        "        if not self.hidden_nodes: return 0, 0\n",
        "        sample_size = min(sample_size, len(self.hidden_nodes))\n",
        "        sampled_nodes = np.random.choice(self.hidden_nodes, size=sample_size, replace=False)\n",
        "        for node in sampled_nodes:\n",
        "            h_tse, h_sie = self.calculate_metrics_for_node(node)\n",
        "            if np.isfinite(h_tse) and np.isfinite(h_sie):\n",
        "                htse_vals.append(h_tse); hsie_vals.append(h_sie)\n",
        "        return np.mean(htse_vals) if htse_vals else 0, np.mean(hsie_vals) if hsie_vals else 0\n",
        "\n",
        "# --- 3. Core Experiment and Plotting Functions ---\n",
        "\n",
        "def run_single_sweep(seed, full_train_dataset, device):\n",
        "    \"\"\"Executes a single full N-Sweep for a given seed.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    indices = torch.randperm(len(full_train_dataset))[:CONFIG[\"dataset_size\"]]\n",
        "    train_subset = Subset(full_train_dataset, indices)\n",
        "    train_loader = DataLoader(train_subset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
        "\n",
        "    run_results = []\n",
        "    pbar = tqdm(CONFIG[\"hidden_size_sweep\"], desc=f\"Running Sweep for Seed {seed}\", leave=False)\n",
        "    for hidden_size in pbar:\n",
        "        model = SimpleMLP(hidden_size=hidden_size).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(CONFIG[\"fixed_epochs\"]):\n",
        "            for data, target in train_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        analyzer = TheoryAnalyzer(model)\n",
        "        final_htse, final_hsie = analyzer.analyze_model_state(CONFIG[\"analysis_sample_size\"])\n",
        "\n",
        "        run_results.append({\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"final_htse\": final_htse,\n",
        "            \"final_hsie\": final_hsie\n",
        "        })\n",
        "    return run_results\n",
        "\n",
        "# --- 4. Main Execution Block ---\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    full_train_dataset = datasets.FashionMNIST('.', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "    all_runs_results = []\n",
        "    for i in range(CONFIG[\"num_runs\"]):\n",
        "        current_seed = CONFIG[\"base_seed\"] + i\n",
        "        print(f\"\\n{'='*20} Starting Run {i+1}/{CONFIG['num_runs']} with Seed {current_seed} {'='*20}\")\n",
        "        single_run_data = run_single_sweep(current_seed, full_train_dataset, device)\n",
        "        all_runs_results.append(single_run_data)\n",
        "\n",
        "    with open(CONFIG[\"results_filename\"], 'w') as f:\n",
        "        json.dump(all_runs_results, f, indent=4)\n",
        "    print(f\"\\nRaw experiment data for all {CONFIG['num_runs']} runs saved to: {CONFIG['results_filename']}\")\n",
        "    print(\"Experiment complete. You can now use the updated plotting script to visualize the aggregated results.\")"
      ]
    }
  ]
}