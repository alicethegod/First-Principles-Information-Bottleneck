{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"n_sweep_ib_verifier.py\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1w_obf9T5KkYAc3zd7x9zU9Xrlh2NegGi\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Information Bottleneck (IB) Trajectory Verification via N-Sweep\n",
        "\n",
        "This script implements a novel and theoretically sound experiment to demonstrate\n",
        "the Information Bottleneck phenomenon.\n",
        "\n",
        "Hypothesis (v3 - The Correct Interpretation):\n",
        "The IB dynamic does not correspond to a monotonic increase in data (D), but\n",
        "rather to a change in the internal resources (N) allocated to a concept.\n",
        "- The Fitting Phase is analogous to INCREASING the model capacity (N) to\n",
        "  capture all details of a fixed task. This leads to a decrease in H'_tse\n",
        "  (more shortcuts) and an increase in H'_sie (more redundancy).\n",
        "- The Compression Phase is analogous to subsequently DECREASING the model\n",
        "  capacity (N), forcing the system to find a more general, sparse, and\n",
        "  \"economical\" representation. This leads to an increase in H'_tse (shortcuts\n",
        "  are removed) and a decrease in H'_sie (redundancy is pruned).\n",
        "\n",
        "This script simulates this process by training a series of models with\n",
        "capacities (N) that first increase and then decrease, on a fixed dataset.\n",
        "The resulting trajectory of final states should trace the classic IB curve.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "import copy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "CONFIG = {\n",
        "    \"seed\": 42,\n",
        "    \"fixed_epochs\": 40,\n",
        "    \"dataset_size\": 10000, # Fixed dataset size\n",
        "    \"hidden_size_sweep\": [4, 6, 8, 12, 16, 24, 32, 48, 64, 96, 128, 96, 64, 48, 32, 24, 16, 12, 8, 6, 4],\n",
        "    \"batch_size\": 256,\n",
        "    \"learning_rate\": 0.002,\n",
        "    \"analysis_sample_size\": 50,\n",
        "    \"results_filename\": \"n_sweep_ib_results.json\",\n",
        "    \"figure_filename\": \"n_sweep_ib_summary_plot.png\"\n",
        "}\n",
        "\n",
        "# --- 2. Model and Theory Analyzer Definition ---\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_size=10, num_classes=10):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x.view(-1, 784))\n",
        "\n",
        "class TheoryAnalyzer:\n",
        "    # This class remains the same as in previous scripts.\n",
        "    def __init__(self, model):\n",
        "        model_copy = copy.deepcopy(model); model_copy.eval()\n",
        "        self.model = model_copy.to('cpu')\n",
        "        self.linear_layers = [m for m in self.model.modules() if isinstance(m, nn.Linear)]\n",
        "        self.graph = self._build_graph()\n",
        "        self.grounding_nodes = self._get_grounding_nodes()\n",
        "        self.hidden_nodes = self._get_hidden_nodes()\n",
        "        self.memoized_paths = {}\n",
        "    def _build_graph(self):\n",
        "        G = nx.DiGraph(); layers = self.linear_layers;\n",
        "        if not layers: return G\n",
        "        for i in range(layers[0].in_features): G.add_node(f\"0-{i}\", layer=0)\n",
        "        for i, l in enumerate(layers):\n",
        "            for j in range(l.out_features): G.add_node(f\"{i+1}-{j}\", layer=i+1)\n",
        "            weights = torch.abs(l.weight.data.t()); probs = torch.softmax(weights, dim=1)\n",
        "            for u in range(l.in_features):\n",
        "                for v in range(l.out_features):\n",
        "                    p = probs[u, v].item()\n",
        "                    if p > 1e-9: G.add_edge(f\"{i}-{u}\", f\"{i+1}-{v}\", cost=1.0 - np.log(p))\n",
        "        return G\n",
        "    def _get_grounding_nodes(self):\n",
        "        num_linear_layers = len(self.linear_layers)\n",
        "        return {node for node, data in self.graph.nodes(data=True) if data['layer'] == num_linear_layers}\n",
        "    def _get_hidden_nodes(self):\n",
        "        num_linear_layers = len(self.linear_layers)\n",
        "        return [node for node, data in self.graph.nodes(data=True) if data['layer'] in range(1, num_linear_layers)]\n",
        "    def find_all_paths_dfs(self, start, targets):\n",
        "        memo_key = (start, tuple(sorted(list(targets))))\n",
        "        if memo_key in self.memoized_paths: return self.memoized_paths[memo_key]\n",
        "        paths, stack = [], [(start, [start], 0)]\n",
        "        while stack:\n",
        "            curr, path, cost = stack.pop()\n",
        "            if curr in targets: paths.append({'path': path, 'cost': cost}); continue\n",
        "            if len(path) > (len(self.linear_layers) + 2): continue\n",
        "            for neighbor in self.graph.neighbors(curr):\n",
        "                if neighbor not in path:\n",
        "                    new_cost = cost + self.graph[curr][neighbor]['cost']\n",
        "                    stack.append((neighbor, path + [neighbor], new_cost))\n",
        "        self.memoized_paths[memo_key] = paths; return paths\n",
        "    def calculate_metrics_for_node(self, node):\n",
        "        paths = self.find_all_paths_dfs(node, self.grounding_nodes)\n",
        "        if not paths: return float('inf'), float('inf')\n",
        "        costs = np.array([p['cost'] for p in paths])\n",
        "        importances = np.exp(-1.0 * costs); conductances = 1.0 / costs\n",
        "        h_tse = 1.0 / np.sum(conductances) if np.sum(conductances) > 0 else float('inf')\n",
        "        total_importance = np.sum(importances)\n",
        "        probabilities = importances / total_importance if total_importance > 0 else importances\n",
        "        h_sie = -np.sum(probabilities * np.log2(probabilities + 1e-9))\n",
        "        return h_tse, h_sie\n",
        "    def analyze_model_state(self, sample_size):\n",
        "        htse_vals, hsie_vals = [], []\n",
        "        if not self.hidden_nodes: return 0, 0\n",
        "        sample_size = min(sample_size, len(self.hidden_nodes))\n",
        "        sampled_nodes = np.random.choice(self.hidden_nodes, size=sample_size, replace=False)\n",
        "        for node in sampled_nodes:\n",
        "            h_tse, h_sie = self.calculate_metrics_for_node(node)\n",
        "            if np.isfinite(h_tse) and np.isfinite(h_sie):\n",
        "                htse_vals.append(h_tse); hsie_vals.append(h_sie)\n",
        "        return np.mean(htse_vals) if htse_vals else 0, np.mean(hsie_vals) if hsie_vals else 0\n",
        "\n",
        "# --- 3. Core Experiment and Plotting Functions ---\n",
        "\n",
        "def run_experiment():\n",
        "    \"\"\"Executes the N-Sweep and returns results.\"\"\"\n",
        "    torch.manual_seed(CONFIG[\"seed\"])\n",
        "    np.random.seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    transform = transforms.ToTensor()\n",
        "    full_train_dataset = datasets.FashionMNIST('.', train=True, download=True, transform=transform)\n",
        "    indices = torch.randperm(len(full_train_dataset))[:CONFIG[\"dataset_size\"]]\n",
        "    train_subset = Subset(full_train_dataset, indices)\n",
        "    train_loader = DataLoader(train_subset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
        "\n",
        "    results = []\n",
        "    pbar = tqdm(CONFIG[\"hidden_size_sweep\"], desc=\"Sweeping hidden layer size N\")\n",
        "    for hidden_size in pbar:\n",
        "        model = SimpleMLP(hidden_size=hidden_size).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(CONFIG[\"fixed_epochs\"]):\n",
        "            for data, target in train_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        analyzer = TheoryAnalyzer(model)\n",
        "        final_htse, final_hsie = analyzer.analyze_model_state(CONFIG[\"analysis_sample_size\"])\n",
        "\n",
        "        results.append({\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"final_htse\": final_htse,\n",
        "            \"final_hsie\": final_hsie\n",
        "        })\n",
        "        pbar.set_postfix_str(f\"N={hidden_size}, Htse={final_htse:.4f}, Hsie={final_hsie:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def plot_results(results_df):\n",
        "    \"\"\"Generates the final summary plot from the results DataFrame.\"\"\"\n",
        "    results_df['htse_norm'] = minmax_scale(results_df['final_htse'])\n",
        "    results_df['hsie_norm'] = minmax_scale(results_df['final_hsie'])\n",
        "\n",
        "    # Add a step counter for coloring the trajectory\n",
        "    results_df['step'] = range(len(results_df))\n",
        "\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    points = plt.scatter(results_df['htse_norm'], results_df['hsie_norm'],\n",
        "                         c=results_df['step'],\n",
        "                         cmap='viridis', s=80, ec='black', lw=0.5)\n",
        "\n",
        "    plt.plot(results_df['htse_norm'], results_df['hsie_norm'], color='black', alpha=0.4, zorder=0)\n",
        "\n",
        "    # Annotate start and end\n",
        "    plt.text(results_df['htse_norm'].iloc[0], results_df['hsie_norm'].iloc[0], ' Start (N increases)',\n",
        "             fontsize=12, verticalalignment='bottom', horizontalalignment='right')\n",
        "    plt.text(results_df['htse_norm'].iloc[-1], results_df['hsie_norm'].iloc[-1], ' End (N decreases)',\n",
        "             fontsize=12, verticalalignment='top', horizontalalignment='left')\n",
        "\n",
        "    # Annotate turning point\n",
        "    turning_point_idx = np.argmax(results_df['hsie_norm'])\n",
        "    plt.plot(results_df['htse_norm'].iloc[turning_point_idx], results_df['hsie_norm'].iloc[turning_point_idx],\n",
        "             'o', color='red', markersize=15, label='Turning Point (Compression Starts)')\n",
        "\n",
        "\n",
        "    cbar = plt.colorbar(points)\n",
        "    cbar.set_label('N-Sweep Step', fontsize=14)\n",
        "    plt.xlabel(\"Normalized Cognitive Cost ($H'_{tse}$) $\\\\rightarrow$\", fontsize=16)\n",
        "    plt.ylabel(\"Normalized Semantic Robustness ($H'_{sie}$) $\\\\rightarrow$\", fontsize=16)\n",
        "    plt.title(\"Information Bottleneck Trajectory Revealed by N-Sweep\", fontsize=20, pad=20)\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.legend(fontsize=12)\n",
        "\n",
        "    plt.savefig(CONFIG[\"figure_filename\"], dpi=300)\n",
        "    print(f\"\\nSummary plot saved to: {CONFIG['figure_filename']}\")\n",
        "    plt.show()\n",
        "\n",
        "# --- 4. Main Execution Block ---\n",
        "if __name__ == '__main__':\n",
        "    results_data = run_experiment()\n",
        "\n",
        "    with open(CONFIG[\"results_filename\"], 'w') as f:\n",
        "        json.dump(results_data, f, indent=4)\n",
        "    print(f\"\\nRaw experiment data saved to: {CONFIG['results_filename']}\")\n",
        "\n",
        "    if results_data:\n",
        "        results_df = pd.DataFrame(results_data)\n",
        "        plot_results(results_df)\n",
        "    else:\n",
        "        print(\"No results were generated, skipping plotting.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "gmVdlKHiQ5_X"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}